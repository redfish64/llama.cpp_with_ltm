* we might possibly understand the kv layers by looking here:
static void llama_kv_cache_defrag_internal(struct llama_context & lctx) {


* this is right after the prompt is loaded. We can find the later KV layers
* here and create a vector to lookup in the RAG database.
* Then add thoughts to the prompt and continue in the loop above.
* We can also use this same vector to save the the prompt data, including the user
* input.  
       if ((int) embd_inp.size() <= n_consumed && !is_interacting) {

*** As far as what to place in thoughts? That can be configurable.

**** this is where the model is loaded, in main.cpp
    // load the model and apply lora adapter, if any
    LOG_INF("%s: load the model and apply lora adapter, if any\n", __func__);
    common_init_result llama_init = common_init_from_params(params);

**ggml vectors have operations associated with them, which are stored in the ggml_tensor struct, in ggml.h:

   enum ggml_op {
        GGML_OP_NONE = 0,

        GGML_OP_DUP,
        GGML_OP_ADD,
        GGML_OP_ADD1,
        GGML_OP_ACC,
 

 *** loads the gguf vectors in, which has all the ops and everything. We'd need to insert our crap in the model before running it 
     in order to do anything. llama.cpp:4333
     llama_model_loader(const std::string & fname, bool use_mmap, bool check_tensors, const struct llama_model_kv_override * param_overrides_p) {

*** actual loading of the vectors ggml.c:7318
    // read the tensor infos
    if (ctx->header.n_tensors > 0) {
        ctx->infos = calloc(ctx->header.n_tensors, sizeof(struct gguf_tensor_info));
 
-- model.ctxs what is it?

202	    model = llama_init.model;
 *** it appears that the model is loaded with tensors with zero ops. This would make sense because I 
 dont see it in the gguf format
 https://github.com/ggerganov/ggml/blob/master/docs/gguf.md

 https://huggingface.co/blog/introduction-to-ggml

 *** after every decode (prompt and then every token in output), it calls:
 10625	    struct ggml_cgraph * build_llama() {



static struct ggml_cgraph * llama_build_graph(
*** this contains a function called cb() which is run for every tensor created in the tensor graph
    we'd probably want to put our hook here.


llama_decode_internal
17496	            float * logits_out = lctx.logits + n_outputs_prev*n_vocab;
*** so something is setting the lctx.logits'


*** ggml
everytime in examples, uses two contexts, a ggml_context for the input, and another ggml_context for the graph.
llama.cpp also does this: struct ggml_context * ctx0 = nullptr;


When llama.cpp creates a graph it uses:     
struct ggml_cgraph * gf = ggml_new_graph_custom(ctx0, llama_model_max_nodes(model), false);

This is just like ggml_new_graph but changes the size of the context from the default. I imagine this 
expands if too small or something like that. (it sets it to max(8192,5*nodes))

*** output
we're creating an offshoot from the main result. I haven't seen anything yet that does this

** cb is only called when making the graph, not when executing, so there is no data
which means we have to finish making the graph, and somehow have 2 outputs, not just one result
Verified this is possible. Just call llama_build_graph twice

** claude generated code has a lot of mistakes

I want to get it running quickly, so I think I just need to update llama.cpp to create a new result

** I think I'm going to skip vector dim reduction for now

First, because I don't know if it would help. Second because thats something that can be experimented with
later. I was thinking of just splitting the vector into multiple components, but just searching against the
whole vector would in effect do this anyway.

So we'll see. The main point is that keeping all the data would be best, at first. We can always update
the vector db later.

** res and emb are outputs and created in llama_build_graph

        ggml_cgraph * gf = llama_build_graph(lctx, ubatch, false);

        // the output is always the last tensor in the graph
        struct ggml_tensor * res  = ggml_graph_node(gf, -1);
        struct ggml_tensor * embd = ggml_graph_node(gf, -2);

 ** The question is, how do we make sure not to disturb the kv cache among other things 
 when we make our own llama_build_graph for the upper layers.

 According to claude, kv cache uses a separate graph, so it won't interfere

 To get the graph to store the output, we need to run:
 ggml_build_forward_expand

 ** to get the tensor value out, call this:

         void (*get_tensor_async)(ggml_backend_t backend, const struct ggml_tensor * tensor,       void * data, size_t offset, size_t size);
    I don't know when it syncs??? or how. Maybe automatic???